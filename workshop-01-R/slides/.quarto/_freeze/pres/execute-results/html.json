{
  "hash": "d13c277e18ee0977d7e858b1474dcf73",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"HPC Workshop (R)\"\nformat: revealjs\n---\n\n\n\n## HPC Workshop (R)\n\nWelcome to this workshop on High Performance Computing (HPC) using the University of Washington's Hyak cluster. This workshop is designed to give you a basic understanding of how to use the cluster for your research using a practical example of running a computationally intensive bootstrap for data analysis.\n\n## Aims\n\n-   Understand how HPC can accelerate your research\n-   Learn how to log in to Hyak and navigate the file system\n-   Learn how to submit jobs to the cluster and manage them\n-   Wrap your analysis in an Apptainer container for reproducibility and portability\n-   See how you can use array jobs to accelerate your analysis\n\n## More resources\n\n-   <https://uw-psych.github.io/compute_docs>\n-   <https://hyak.uw.edu/docs>\n-   <https://www.hpc-carpentry.org>\n\n## What is an HPC cluster?\n\n-   A **cluster** is a group of computers that work together to solve problems\n-   An **HPC cluster** is a cluster of computers that are optimized for running computations that require a lot of CPU power and memory\n-   The **Hyak** cluster at the University of Washington has hundreds of computers that are connected together and can be used to run computations in parallel\n-   More than 21,000 CPU cores and 1,000 GPUs\n\n## Why use an HPC cluster?\n\nSpeed!\n\n-   **CPU power**: Multiple high-performance CPUs per node (56 on `gpu-a40` nodes)\n-   **Memory**: Oodles of RAM (1 TB on `gpu-a40` nodes)\n-   **GPU**: Some nodes have GPUs, serious speed increases for certain types of computations (e.g., machine learning)\n-   **Parallelism**: Run computations in parallel across many different computers\n\n## Prerequisites\n\nBefore you can log in to Hyak, you will need:\n\n-   A UW NetID\n-   A SSH client (e.g. PuTTY, MobaXterm, or the terminal on Mac/Linux) on your laptop\n\nIf you have not set up an SSH client, visit <https://uw-psych.github.io/compute_docs/docs/start/connect-ssh.html> for instructions.\n\n## Logging in to Hyak\n\nTo log in to Hyak, open your SSH client and enter the following command:\n\n``` bash\nssh your-uw-netid@klone.hyak.uw.edu\n```\n\nReplace `your-uw-netid` with your UW NetID and enter your credentials when prompted.\n\n## The file system\n\nThe file system on Hyak is organized as a hierarchy:\n\n-   `/` - the root directory\n    -   `/mmfs1` - the main user file system for Hyak\n        -   `/mmfs1/home` - the root of the home directory for all users\n            -   `/mmfs1/home/your-uw-netid` - your home directory (only 10 GB!)\n    -   `/gscratch` - data directory for Hyak users\n        -   `/gscratch/scrubbed` - a directory for temporary files that are periodically deleted\n\n## Your home directory (`~`)\n\nOnce you have logged in, you will be in your **home directory** on the Hyak cluster.\n\n-   Home directory is `C:\\Users\\You` on Windows or `/Users/You` on Mac\n-   Stored under `/mmfs1/home/your-uw-netid`\n-   Can use `~` for short in the command prompt\n\n::: callout-caution\nYour directory is limited to 10 GB. Do not store large files here! Use the `/gscratch` directory instead.\n:::\n\n## Listing files\n\nList the contents of your `~` by typing `ls` into the command prompt and pressing `Enter`:\n\n``` bash\nls\n```\n\nMove to another directory with `cd`, e.g.:\n\n``` bash\ncd /gscratch # [C]hange [d]irectory to /gscratch\nls # List contents\ncd ~ # Move back to home\npwd # Display current directory\n```\n\n## The shell\n\n-   `bash` is the command-line interface we have been using\n-   Both an interpreter and a programming language\n    -   Use interactively to run commands\n    -   Write scripts to automate tasks\n\n## bash\n\n### Comments\n\nIn `bash`, anything after `#` is a comment (like Python, R).\n\n### Environment variables\n\n**Environment variables** help pass inputs to a script. Set environment variables with:\n``` bash\n# Make sure to use quotes and do not put spaces aside '=':\nVARIABLE=\"Something like this\" \n# Print the value to the screen:\necho \"$VARIABLE\" \n# More precise syntax - helps avoid some wild issues:\necho \"${VARIABLE}\" \n # Make VARIABLE available to subsequent external commands:\nexport VARIABLE\n```\n\n## bash: History and Completion\n\n-   Use `↑` and `↓` keys to recall the text of the commands you have run before\n-   Use `Tab` to complete file names, commands etc.\n    -   e.g., `cd /gscr` + `Tab` → `cd /gscratch`\n\n## Editing files\n\nUse a text editor to edit text files (scripts, etc.)\n\n::: columns\n::: {.column width=\"40%\"}\n-   `nano` is a good one if you've never tried one before\n:::\n\n::: {.column width=\"60%\"}\n![](nano.png)\n:::\n:::\n\n::: notes\n-   To start `nano`, type in the command `nano`. You can then start typing text into the editor.\n-   To save your work, press `Ctrl` + `O`, type in a file name, and press `Enter`.\n-   To exit `nano`, press `Ctrl` + `X`.\n:::\n\n## Viewing files\n\nUse:\n\n-   `cat` to view short files\n-   Use `more` to view long files\n    -   ...or `less`\n\n``` bash\ncat /etc/os-release # A short file\nmore /etc/slurm.conf # A long file\nless /etc/slurm.conf # Another way for a long file\n```\n\n## Getting help with commands\n\n-   `man` displays the manual page (\"manpage\") for a command\n-   `man ls` - displays the manual page for the `ls` command.\n-   \"manpages\" tend to be exhaustive and overwhelming!\n-   Add `--help` to a command to get a shorter, more user-friendly help message for many commands\n\n## `tldr` for quick command reference\n\n`tldr`: A supplement to `man` pages providing practical examples:\n\n``` bash\npip3.9 install --user tldr\ntldr ls\n```\n\n![](tldr.png)\n\n## `ranger`\n\n`ranger` is an easy-to-use program to navigate the file system:\n\n``` bash\npip3.9 install --user ranger-fm\n```\n\n::: columns\n::: {.column width=\"30%\"}\n-   Navigate using arrow keys\n-   Open files with `Enter`\n-   Exit with `q`\n:::\n\n::: {.column width=\"70%\"}\n![](ranger-fm.png)\n:::\n:::\n\n## SLURM\n\n-   Jobs: programs/scripts you want to run + resources allocated for them\n-   Jobs on Hyak are scheduled using the SLURM **workload manager**\n-   Specify the resources you need when submitting a job\n-   Job runs when SLURM determines there are enough resources available\n-   Submitting a job runs it on the cluster when the resources are available\n\n::: notes\nJobs are programs or scripts that you want to run on the cluster. You submit jobs to SLURM, and it schedules and runs them on the cluster when resources are available. Resource allocation depends on the amount of resources you request, the resources available on the cluster, and the resources available to the SLURM account you are using.\n:::\n\n## Login & Compute Nodes\n\n-   The **login node** is the computer you are connected to after running `SSH`\n-   Use the login node for submitting and managing jobs, minor tasks like editing a script or copying a handful of files\n-   Do **not** use it to run your computations\n-   **Compute nodes** are where your jobs will run\n-   The scheduler will allocate resources on the compute nodes to your jobs\n-   Jobs can be run in parallel on multiple compute nodes\n\n## Resource availability\n\nThe main resources you will be concerned with are:\n\n-   **CPUs** - the number of CPU cores you can use\n-   **Memory** - the amount of RAM you can use\n-   **GPUs** - the number of GPUs you can use\n\n## hyakalloc\n\n-   `hyakalloc` shows the resources available to you across all the nodes on the cluster\n\n::: columns\n::: {.column width=\"40%\"}\n-   A job made to run on a single node will have to wait for all the resources it needs to become available on a single node\n:::\n\n::: {.column width=\"60%\"}\n![](hyakalloc.png)\n:::\n:::\n\n## The queue\n\n-   Jobs are submitted to a **queue** in SLURM\n-   Use `squeue` to see the jobs in the queue that are running or waiting to run\n\n``` bash\nsqueue\n```\n\n-   Use `squeue --me` to see only your jobs in the queue\n\n``` bash\nsqueue --me\n```\n\n## Interactive session\n\n-   An **interactive session** is a way to get access to a compute node for a short period of time\n-   Use an interactive session to test your code, run small jobs, or debug problems\n-   Use the `salloc` command to start an interactive session\n\n## Launching an interactive session\n\nTo launch a job, you will need to specify the resources you need, the account to charge the resources to, and the partition -- group of resources -- to run the job on.\n\nFor a session using the `escience` account, the `gpu-a40` partition, 1 hour of time, 1G of memory, 1 CPU:\n\n``` bash\nsalloc \\\n    --account escience \\\n    --partition gpu-a40 \\\n    --time=1:00:00 \\\n    --mem=1G \\\n    --cpus-per-task=1\n```\n\nYou may have to wait for resources to become available -- use `squeue` to check the status of your request.\n\n## Running commands in an interactive session\n\nWhen your interactive session starts, you will be given a prompt on a compute node where you can run commands and test your code. For example, you can run the `hostname` command to see the name of the compute node you are on:\n\n``` bash\nhostname\n```\n\nAny commands you run in the interactive session will be run on the compute node you are on and will not affect the login node.\n\n\n## Apptainer\n\n- We need to use Apptainer (f.k.a. Singularity) to run R on Hyak\n- Apptainer is a tool for creating and running software containers (like Docker, Podman)\n- Containers help you package an application and its dependencies so that it can run consistently (and reproducibly) across different environments\n- An Apptainer container is like a lightweight, portable, and self-contained virtual machine -- c.f., VirtualBox, VMware, Parallels, etc.\n\n\n## Running R interactively\n\nIn the interactive session, we can download and run R as follows:\n\n```bash\n# Create the directory to store your things if it isn't there:\nmkdir -p \"/gscratch/scrubbed/$USER\"\n\n# Go to the directory:\ncd \"$_\"\n\n# Download the image to the file r-latest.sif:\napptainer pull --disable-cache r-latest.sif docker://r-base:latest\n\n# Launch R:\napptainer exec --bind /gscratch r-latest.sif R\n```\n\n# Exiting\n\n- Exit R as normal\n- When you're back in the shell, If you run `exit` or `Ctrl` + `D`, the interactive session will end\n\n## Questions?\n\nDo you have any questions about what we've covered so far?\n\n## Running an example analysis\n\n### Practical example\n\n**Obtaining confidence intervals for a point estimate using the bootstrap**\n\n-   The bootstrap is a resampling method that can be used to estimate the sampling distribution of a statistic\n-   Requires running the same analysis many times on different samples of the data\n-   Can be extremely computationally intensive\n-   Many opportunities for running these repeated computations in parallel\n\n## Dataset\n\n-   Personality data from the SAPA project (\"Synthetic Aperture Personality Assessment,\" Revelle, W.) from `psychTools`\n\n\n\n::: {.cell}\n\n:::\n\n\n\n-   2800 rows, 6 cols -- each row an individual subject\n-   1 demographic column (`gender`) with 2 genders coded\n-   5 columns of scored personality scales:\n    -   Agreeableness (`agree`)\n    -   Conscientiousness (`conscientious`)\n    -   Extraversion (`extraversion`)\n    -   Neuroticism (`neuroticism`)\n    -   Openness (`openness`)\n\n## Goal\n\n::: columns\n::: {.column width=\"40%\"}\n-   Estimate the 95% confidence interval for Cohen's *d* of the mean difference among two genders using the bootstrap\n:::\n\n::: {.column width=\"60%\"}\n-   For each category, resample the data with replacement\n-   Calculate Cohen's *d*\n-   Repeat this process many times (e.g., 10,000 times)\n-   Use the distribution to estimate the 95% confidence interval using `boot`\n:::\n:::\n\n## Setup\n\nCreate a new directory for the project under /gscratch/scrubbed/your-uw-netid:\n\n```sh\nmkdir -p \"/gscratch/scrubbed/$USER\"\ncd \"$_\" # Go to the new directory\n```\n\nDownload the project files from GitHub, navigate to the project directory, and list the contents:\n\n``` bash\ngit clone https://github.com/uw-psych/hpc-workshop\ncd hpc-workshop\nls\n```\n\n## Setup (cont.)\n\nWe will use a R script for our computations -- `workshop-01/split-job.py`:\n\n``` bash\ncd workshop-01-R\nls # List contents of directory\nnano split-job.R # View the contents of the file\n```\n\n## Batch jobs\n\n-   Can't use an interactive session for this -- need to submit a batch job\n-   `sbatch` command submits a batch job to SLURM\n-   Commands to run the job are specified in a job script\n-   Job script specifies the resources to request, the commands to run, and the environment variables to set\n-   `workshop-01-R/sbatch-r.job` contains our job script\n\n## The job script\n\nHave a look at the job script in `sbatch-r.job`. We will submit this to `sbatch`, which will schedule and launch our tasks.\n\n## Job metadata and resources\n\nThe following defines metadata and resources to request to run the job. These parameters are defined with `#SBATCH`:\n\n```sh\n#!/usr/bin/env bash\n#SBATCH --output=job.log\n#SBATCH --job-name=psych_bfi_boot\n#SBATCH --nodes=5\n#SBATCH --mem=8G\n#SBATCH --time=1:00:00\n```\n\n## Environment variables\n\nWe use an **environment variable** to define where we want to store the Apptainer image file and how many iterations to run:\n\n```sh\n#\tThe parameters to apptainer are specified by environment variables.\n#\tThe commands below will use the default values specified below unless\n#\tthese environment variables are already set. (this is what the :- means)\n#\t\tR_IMAGE:\n#\t\t\tThe path to the apptainer image to use\nR_IMAGE=\"/gscratch/scrubbed/${USER}/r-latest.sif\"\n\n# Create the base directory for the image:\nmkdir -p \"$(dirname \"R_IMAGE\")\"\n```\n\n## Environment variables (cont.)\n\nWe use another **environment variable** to define how many iterations to run:\n```sh\n# SCRIPT SETUP:\n# The parameters to the R script cam be specified by environment variables.\n# They must be exported so that they are visible to the R script.\n#\tThe commands below will use the default values specified below unless\n#\tthese environment variables are already set. (this is what the :- means)\n#\t\tBOOT_ITER:\n#\t\t\tThe number of bootstrap iterations to run\nexport BOOT_ITER=\"${BOOT_ITER:-10000}\"\n```\n\n## Downloading the image \n\nThe following downloads the image to the location `$R_IMAGE`, unless it exists already (which `||` allows):\n\n```sh\napptainer pull --disable-cache \\\n  \"$R_IMAGE\" docker://r-base:latest 2>/dev/null \\\n    || echo \"Image already exists. Continuing.\" >&2\n```\n\n## Running the jobs\n\nHere, we launch a job for each category we want to compute on:\n\n```sh\nfor x in \"agree\" \"conscientious\" \"extraversion\"  \"neuroticism\" \"openness\"; do\n  echo \"Launching job for $x\" >&2\n  srun -N1 -n1 --exclusive \\\n    apptainer exec \\\n      --bind /gscratch \\\n      \"$R_IMAGE\" \\\n      Rscript split-job.R ../data/psych_bfi_scored.csv \"$x\" &\ndone\n```\n\n## Combining the output\n\nOnce the job is done, we need to assemble the output. We have a separate script for that -- `merge-outputs.R`:\n\n```sh\nsrun -N1 -n1 --exclusive \\\n  apptainer exec \\\n    --bind /gscratch \\\n    \"$R_IMAGE\" \\\n    Rscript merge-outputs.R output/combined.csv output/*.csv\n```\n\n## Submitting the job\n\nNavigate to the `workshop-01-R` directory if you're not already there:\n\n``` bash\ncd /gscratch/scrubbed/$USER/hpc-workshop/workshop-01-R\n```\n\nSubmit an array job to SLURM using the `sbatch` command, using the `ckpt` paritition:\n\n``` bash\nsbatch --account escience --partition ckpt sbatch.job\n```\n\n## Monitoring the job\n\nAs usual, you can use the `squeue` command to monitor the status of your job:\n\n``` bash\nsqueue --me\n```\n\nBut there's another way you can track progress in real time...\n\n## Log files\n\n- `sbatch-r.job` specifies a location for the output files and the log files for the array job\n-   Files with the extension `.log` contain what the job would print to the screen\n-   Read log files to track the progress of the individual jobs in the array with `tail -f`:\n\n```bash\ntail -f job.log\n```\n\nPress `Ctrl` + `D` to stop following the log files.\n\n## Getting the results\n\nUse `scp` to copy the results from the cluster to your local machine:\n\n`scp -r your-uw-netid@klone.hyak.uw.edu:/gscratch/scrubbed/your-uw-netid/hpc-workshop/workshop-01/output/combined.csv .`\n\n## Q&A\n\nQuestions?\n\n## More resources\n\n-   <https://uw-psych.github.io/compute_docs>\n-   <https://hyak.uw.edu/docs>\n-   <https://www.hpc-carpentry.org>\n",
    "supporting": [
      "pres_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}