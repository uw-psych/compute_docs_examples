---
title: "HPC Workshop (R)"
format: revealjs
---

## HPC Workshop (R)

Welcome to this workshop on High Performance Computing (HPC) using the University of Washington's Hyak cluster. This workshop is designed to give you a basic understanding of how to use the cluster for your research using a practical example of running a computationally intensive bootstrap for data analysis.

## Aims

-   Understand how HPC can accelerate your research
-   Learn how to log in to Hyak and navigate the file system
-   Learn how to submit jobs to the cluster and manage them
-   Wrap your analysis in an Apptainer container for reproducibility and portability
-   See how you can use array jobs to accelerate your analysis

## More resources

-   <https://uw-psych.github.io/compute_docs>
-   <https://hyak.uw.edu/docs>
-   <https://www.hpc-carpentry.org>

## What is an HPC cluster?

-   A **cluster** is a group of computers that work together to solve problems
-   An **HPC cluster** is a cluster of computers that are optimized for running computations that require a lot of CPU power and memory
-   The **Hyak** cluster at the University of Washington has hundreds of computers that are connected together and can be used to run computations in parallel
-   More than 21,000 CPU cores and 1,000 GPUs

## Why use an HPC cluster?

Speed!

-   **CPU power**: Multiple high-performance CPUs per node (56 on `gpu-a40` nodes)
-   **Memory**: Oodles of RAM (1 TB on `gpu-a40` nodes)
-   **GPU**: Some nodes have GPUs, serious speed increases for certain types of computations (e.g., machine learning)
-   **Parallelism**: Run computations in parallel across many different computers

## Prerequisites

Before you can log in to Hyak, you will need:

-   A UW NetID
-   A SSH client (e.g. PuTTY, MobaXterm, or the terminal on Mac/Linux) on your laptop

If you have not set up an SSH client, visit <https://uw-psych.github.io/compute_docs/docs/start/connect-ssh.html> for instructions.

## Logging in to Hyak

To log in to Hyak, open your SSH client and enter the following command:

``` bash
ssh your-uw-netid@klone.hyak.uw.edu
```

Replace `your-uw-netid` with your UW NetID and enter your credentials when prompted.

## The file system

The file system on Hyak is organized as a hierarchy:

-   `/` - the root directory
    -   `/mmfs1` - the main user file system for Hyak
        -   `/mmfs1/home` - the root of the home directory for all users
            -   `/mmfs1/home/your-uw-netid` - your home directory (only 10 GB!)
    -   `/gscratch` - data directory for Hyak users
        -   `/gscratch/scrubbed` - a directory for temporary files that are periodically deleted

## Your home directory (`~`)

Once you have logged in, you will be in your **home directory** on the Hyak cluster.

-   Home directory is `C:\Users\You` on Windows or `/Users/You` on Mac
-   Stored under `/mmfs1/home/your-uw-netid`
-   Can use `~` for short in the command prompt

::: callout-caution
Your directory is limited to 10 GB. Do not store large files here! Use the `/gscratch` directory instead.
:::

## Listing files

List the contents of your `~` by typing `ls` into the command prompt and pressing `Enter`:

``` bash
ls
```

Move to another directory with `cd`, e.g.:

``` bash
cd /gscratch # [C]hange [d]irectory to /gscratch
ls # List contents
cd ~ # Move back to home
pwd # Display current directory
```

## The shell

-   `bash` is the command-line interface we have been using
-   Both an interpreter and a programming language
    -   Use interactively to run commands
    -   Write scripts to automate tasks

## bash

### Comments

In `bash`, anything after `#` is a comment (like Python, R).

### Environment variables

**Environment variables** help pass inputs to a script. Set environment variables with:
``` bash
# Make sure to use quotes and do not put spaces aside '=':
VARIABLE="Something like this" 
# Print the value to the screen:
echo "$VARIABLE" 
# More precise syntax - helps avoid some wild issues:
echo "${VARIABLE}" 
 # Make VARIABLE available to subsequent external commands:
export VARIABLE
```

## bash: History and Completion

-   Use `↑` and `↓` keys to recall the text of the commands you have run before
-   Use `Tab` to complete file names, commands etc.
    -   e.g., `cd /gscr` + `Tab` → `cd /gscratch`

## Editing files

Use a text editor to edit text files (scripts, etc.)

::: columns
::: {.column width="40%"}
-   `nano` is a good one if you've never tried one before
:::

::: {.column width="60%"}
![](nano.png)
:::
:::

::: notes
-   To start `nano`, type in the command `nano`. You can then start typing text into the editor.
-   To save your work, press `Ctrl` + `O`, type in a file name, and press `Enter`.
-   To exit `nano`, press `Ctrl` + `X`.
:::

## Viewing files

Use:

-   `cat` to view short files
-   Use `more` to view long files
    -   ...or `less`

``` bash
cat /etc/os-release # A short file
more /etc/slurm.conf # A long file
less /etc/slurm.conf # Another way for a long file
```

## Getting help with commands

-   `man` displays the manual page ("manpage") for a command
-   `man ls` - displays the manual page for the `ls` command.
-   "manpages" tend to be exhaustive and overwhelming!
-   Add `--help` to a command to get a shorter, more user-friendly help message for many commands

## `tldr` for quick command reference

`tldr`: A supplement to `man` pages providing practical examples:

``` bash
pip3.9 install --user tldr
tldr ls
```

![](tldr.png)

## `ranger`

`ranger` is an easy-to-use program to navigate the file system:

``` bash
pip3.9 install --user ranger-fm
```

::: columns
::: {.column width="30%"}
-   Navigate using arrow keys
-   Open files with `Enter`
-   Exit with `q`
:::

::: {.column width="70%"}
![](ranger-fm.png)
:::
:::

## SLURM

-   Jobs: programs/scripts you want to run + resources allocated for them
-   Jobs on Hyak are scheduled using the SLURM **workload manager**
-   Specify the resources you need when submitting a job
-   Job runs when SLURM determines there are enough resources available
-   Submitting a job runs it on the cluster when the resources are available

::: notes
Jobs are programs or scripts that you want to run on the cluster. You submit jobs to SLURM, and it schedules and runs them on the cluster when resources are available. Resource allocation depends on the amount of resources you request, the resources available on the cluster, and the resources available to the SLURM account you are using.
:::

## Login & Compute Nodes

-   The **login node** is the computer you are connected to after running `SSH`
-   Use the login node for submitting and managing jobs, minor tasks like editing a script or copying a handful of files
-   Do **not** use it to run your computations
-   **Compute nodes** are where your jobs will run
-   The scheduler will allocate resources on the compute nodes to your jobs
-   Jobs can be run in parallel on multiple compute nodes

## Resource availability

The main resources you will be concerned with are:

-   **CPUs** - the number of CPU cores you can use
-   **Memory** - the amount of RAM you can use
-   **GPUs** - the number of GPUs you can use

## hyakalloc

-   `hyakalloc` shows the resources available to you across all the nodes on the cluster

::: columns
::: {.column width="40%"}
-   A job made to run on a single node will have to wait for all the resources it needs to become available on a single node
:::

::: {.column width="60%"}
![](hyakalloc.png)
:::
:::

## The queue

-   Jobs are submitted to a **queue** in SLURM
-   Use `squeue` to see the jobs in the queue that are running or waiting to run

``` bash
squeue
```

-   Use `squeue --me` to see only your jobs in the queue

``` bash
squeue --me
```

## Interactive session

-   An **interactive session** is a way to get access to a compute node for a short period of time
-   Use an interactive session to test your code, run small jobs, or debug problems
-   Use the `salloc` command to start an interactive session

## Launching an interactive session

To launch a job, you will need to specify the resources you need, the account to charge the resources to, and the partition -- group of resources -- to run the job on.

For a session using the `escience` account, the `gpu-a40` partition, 1 hour of time, 1G of memory, 1 CPU:

``` bash
salloc \
    --account escience \
    --partition gpu-a40 \
    --time=1:00:00 \
    --mem=1G \
    --cpus-per-task=1
```

You may have to wait for resources to become available -- use `squeue` to check the status of your request.

## Running commands in an interactive session

When your interactive session starts, you will be given a prompt on a compute node where you can run commands and test your code. For example, you can run the `hostname` command to see the name of the compute node you are on:

``` bash
hostname
```

Any commands you run in the interactive session will be run on the compute node you are on and will not affect the login node.


## Apptainer

- We need to use Apptainer (f.k.a. Singularity) to run R on Hyak
- Apptainer is a tool for creating and running software containers (like Docker, Podman)
- Containers help you package an application and its dependencies so that it can run consistently (and reproducibly) across different environments
- An Apptainer container is like a lightweight, portable, and self-contained virtual machine -- c.f., VirtualBox, VMware, Parallels, etc.


## Running R interactively

In the interactive session, we can download and run R as follows:

```bash
# Create the directory to store your things if it isn't there:
mkdir -p "/gscratch/scrubbed/$USER"

# Go to the directory:
cd "$_"

# Download the image to the file r-latest.sif:
apptainer pull --disable-cache r-latest.sif docker://r-base:latest

# Launch R (making sure to add /gscratch so the container can access it):
apptainer exec --bind /gscratch r-latest.sif R
```

# Exiting

- Exit R as normal
- When you're back in the shell, If you run `exit` or `Ctrl` + `D`, the interactive session will end

## Questions?

Do you have any questions about what we've covered so far?

## Running an example analysis

### Practical example

**Obtaining confidence intervals for a point estimate using the bootstrap**

-   The bootstrap is a resampling method that can be used to estimate the sampling distribution of a statistic
-   Requires running the same analysis many times on different samples of the data
-   Can be extremely computationally intensive
-   Many opportunities for running these repeated computations in parallel

## Dataset

-   Personality data from the SAPA project ("Synthetic Aperture Personality Assessment," Revelle, W.) from `psychTools`

```{r, echo=FALSE}
library(tidyverse, quietly = TRUE)
library(magrittr, quietly = TRUE)
options(tidyverse.quiet = TRUE)
f <- readr::read_csv("../../data/psych_bfi_scored.csv", progress = FALSE, show_col_types = FALSE)
```

-   `r nrow(f)` rows, `r ncol(f)` cols -- each row an individual subject

## Dataset

-   `r nrow(f)` rows, `r ncol(f)` cols -- each row an individual subject
-   1 demographic column (`gender`) with 2 genders coded

-   1 demographic column (`gender`) with 2 genders coded
-   5 columns of scored personality scales:
    -   Agreeableness (`agree`)
    -   Conscientiousness (`conscientious`)
    -   Extraversion (`extraversion`)
    -   Neuroticism (`neuroticism`)
    -   Openness (`openness`)

## Goal

::: columns
::: {.column width="40%"}
-   Estimate the 95% confidence interval for Cohen's *d* of the mean difference among two genders using the bootstrap
:::

::: {.column width="60%"}
-   For each category, resample the data with replacement
-   Calculate Cohen's *d*
-   Repeat this process many times (e.g., 10,000 times)
-   Use the distribution to estimate the 95% confidence interval using `boot`
:::
:::

## Setup

Create a new directory for the project under /gscratch/scrubbed/your-uw-netid:

```sh
mkdir -p "/gscratch/scrubbed/$USER"
cd "$_" # Go to the new directory
```

Download the project files from GitHub, navigate to the project directory, and list the contents:

``` bash
git clone https://github.com/uw-psych/hpc-workshop
cd hpc-workshop
ls
```

## Setup (cont.)

We will use a R script for our computations -- `workshop-01/split-R`:

``` bash
cd workshop-01-R
ls # List contents of directory
nano split-job.R # View the contents of the file
```

## Batch jobs

-   Can't use an interactive session for this -- need to submit a batch job
-   `sbatch` command submits a batch job to SLURM
-   Commands to run the job are specified in a job script
-   Job script specifies the resources to request, the commands to run, and the environment variables to set
-   `workshop-01-R/sbatch-r.job` contains our job script

## The job script

Have a look at the job script in `sbatch-r.job`. We will submit this to `sbatch`, which will schedule and launch our tasks.

## Job metadata and resources

The following defines metadata and resources to request to run the job. These parameters are defined with `#SBATCH`:

```sh
#!/usr/bin/env bash
#SBATCH --output=job.log
#SBATCH --job-name=psych_bfi_boot
#SBATCH --nodes=5
#SBATCH --mem=8G
#SBATCH --time=1:00:00
```

## Environment variables


We use another **environment variable** to define how many iterations to run:
```sh
# SCRIPT SETUP:
# The parameters to the R script cam be specified by environment variables.
# They must be exported so that they are visible to the R script.
#	The commands below will use the default values specified below unless
#	these environment variables are already set. (this is what the :- means)
#		BOOT_ITER:
#			The number of bootstrap iterations to run
export BOOT_ITER="${BOOT_ITER:-10000}"
```

## Downloading the image 

The following downloads the image, unless it exists already at the location (which `||` allows):

```sh
apptainer pull --disable-cache \
   "/gscratch/scrubbed/${USER}/r-base_latest.sif" docker://r-base:latest 2>/dev/null \
    || echo "Image already exists. Continuing." >&2
```

## Running the jobs

Here, we launch a job for each category we want to compute on:

```sh
for x in "agree" "conscientious" "extraversion"  "neuroticism" "openness"; do
  echo "Launching job for $x" >&2
  srun -N1 -n1 --exclusive \
    apptainer exec \
      --bind /gscratch \
      "/gscratch/scrubbed/${USER}/r-base_latest.sif" \
      Rscript split-job.R ../data/psych_bfi_scored.csv "$x" &
done
```

## Combining the output

Once the job is done, we need to assemble the output. We have a separate script for that -- `merge-outputs.R`:

```sh
srun -N1 -n1 --exclusive \
  apptainer exec \
    --bind /gscratch \
    "$R_IMAGE" \
    Rscript merge-outputs.R output/combined.csv output/*.csv
```

## Submitting the job

Navigate to the `workshop-01-R` directory if you're not already there:

``` bash
cd /gscratch/scrubbed/$USER/hpc-workshop/workshop-01-R
```

Submit an array job to SLURM using the `sbatch` command, using the `gpu-a40` paritition:

``` bash
sbatch --account escience --partition gpu-a40 sbatch-r.job
```

::: callout-tip
Try the checkpoint partition `ckpt` for really short jobs
:::

## Monitoring the job

As usual, you can use the `squeue` command to monitor the status of your job:

``` bash
squeue --me
```

But there's another way you can track progress in real time...

## Log files

- `sbatch-r.job` specifies a location for the output files and the log files for the array job
-   Files with the extension `.log` contain what the job would print to the screen
-   Read log files to track the progress of the individual jobs in the array with `tail -f`:

```bash
tail -f job.log
```

Press `Ctrl` + `D` to stop following the log files.

## Getting the results

Use `scp` to copy the results from the cluster to your local machine:

`scp -r your-uw-netid@klone.hyak.uw.edu:/gscratch/scrubbed/your-uw-netid/hpc-workshop/workshop-01/output/combined.csv .`

## Q&A

Questions?

## More resources

-   <https://uw-psych.github.io/compute_docs>
-   <https://hyak.uw.edu/docs>
-   <https://www.hpc-carpentry.org>
